{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "64f04b6a-fe54-41ce-92fe-ae94fc587387",
   "metadata": {
    "id": "64f04b6a-fe54-41ce-92fe-ae94fc587387"
   },
   "source": [
    "# 1. Import Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e5914e7-a8cb-4a46-bac1-1387df1b5af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import tensorflow\n",
    "import tensorflow as tf\n",
    "# Import matplotlib for Visualising Data\n",
    "from matplotlib import pyplot as plt\n",
    "# Import numpy for Manipulating Data\n",
    "import numpy as np\n",
    "# Import OS for file handling\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea1635e4-4beb-493d-92c1-b106c806ca70",
   "metadata": {
    "id": "ea1635e4-4beb-493d-92c1-b106c806ca70"
   },
   "source": [
    "# 2. Visualise and Build Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d46b074-2a81-4ba1-aa95-ee74f9044e0c",
   "metadata": {},
   "source": [
    "### 2.1 Define Function to Retrieve Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "06a08426-418e-4421-8148-8f8294f842e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data():\n",
    "    # Define the path to the folder containing your images\n",
    "    folder_path = 'dataset'\n",
    "\n",
    "    # Get a list of all files in the directory and sort them numerically\n",
    "    image_files = sorted([f for f in os.listdir(folder_path)])\n",
    "\n",
    "    # Function to load and preprocess an image\n",
    "    def load_image(image_path):\n",
    "        # Load the raw data from the file as a string\n",
    "        img = tf.io.read_file(image_path)\n",
    "        # Decode it into an image tensor\n",
    "        img = tf.image.decode_png(img, channels=3)\n",
    "        return img\n",
    "\n",
    "    # Create lists to store images and labels\n",
    "    images = []\n",
    "    labels = []\n",
    "\n",
    "    # Load images and create labels\n",
    "    for index, filename in enumerate(image_files):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        img = load_image(image_path)\n",
    "        images.append(img)\n",
    "        labels.append(index + 1)  # Labels are 1-based indices\n",
    "\n",
    "    # Convert lists to tensors\n",
    "    images = tf.stack(images)\n",
    "    labels = tf.convert_to_tensor(labels)\n",
    "\n",
    "    # Create a tf.data.Dataset\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((images, labels))\n",
    "\n",
    "    # Store the dataset in a variable called ds\n",
    "    ds = dataset.map(lambda img, lbl: {'image': img, 'label': lbl})\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec631ea8-e4d5-4dfb-9b5c-86d0941c547b",
   "metadata": {},
   "source": [
    "### 2.2 Define Functions to Visualise, and scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b0c62caf-e406-4d12-af31-6f4848155844",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to visualise Data\n",
    "def visualise_dataset(dataiterator):\n",
    "    fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "    # Loop four times and get images \n",
    "    for idx in range(4): \n",
    "        # Grab an image and label\n",
    "        sample = dataiterator.next()\n",
    "        # Plot the image using a specific subplot \n",
    "        ax[idx].imshow(np.squeeze(sample['image']))\n",
    "        # Appending the image label as the plot title \n",
    "        ax[idx].title.set_text(sample['label'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "deb5fca0-fd8a-4557-9c72-1a60c289a2e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to Visualise Generated Images\n",
    "def visualise_generated_images(img):\n",
    "    fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "    # Loop four times and get images \n",
    "    for idx, img in enumerate(img): \n",
    "        # Plot the image using a specific subplot \n",
    "        ax[idx].imshow(np.squeeze(img))\n",
    "        # Appending the image label as the plot title \n",
    "        ax[idx].title.set_text(idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "66c9d901-6a5c-42fd-ad06-cc03f7829728",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale and return images only \n",
    "def scale_images(data): \n",
    "    image = data['image']\n",
    "    image = tf.image.resize(image, [IMG_RES,IMG_RES])\n",
    "    return image / 255\n",
    "\n",
    "# Scale and return dataset\n",
    "def scale_dataset(data): \n",
    "    data['image'] = tf.image.resize(data['image'], [IMG_RES,IMG_RES])\n",
    "    #data['image'] = tf.image.rgb_to_grayscale( data['image'])\n",
    "    data['image'] = data['image'] / 255\n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383d94c4-2d65-4f46-aff2-dfdb3f6d6294",
   "metadata": {},
   "source": [
    "### 2.3 Define constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "380a09d9-bfd5-4648-b9c6-2e564375b93a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a global Scale to control the height and width of dataset images\n",
    "IMG_RES = 28\n",
    "# Define Starting Resolution for images\n",
    "START_RES= int(IMG_RES/4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627958a0-0209-4d14-bf23-bbfb4119163b",
   "metadata": {},
   "source": [
    "### 2.4 Get and Visualise data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "763e752e-a6ed-402b-a072-8b1b3ffe1eb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = get_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbfae672-276d-4a61-8768-4abc4f6442dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup connection aka iterator\n",
    "dataiterator = ds.as_numpy_iterator()\n",
    "# Visualise Pre-scaled Dataset\n",
    "visualise_dataset(dataiterator)\n",
    "\n",
    "# Running the dataset through the scale_images preprocessing step\n",
    "temp = ds.map(scale_dataset) \n",
    "\n",
    "\n",
    "# Setup connection aka iterator\n",
    "dataiterator = temp.as_numpy_iterator()\n",
    "# Check Images after Scaling\n",
    "visualise_dataset(dataiterator)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80d07ec2-a844-42d8-aa32-f5f54cce53f8",
   "metadata": {},
   "source": [
    "### 2.5 PreProcess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "dfc9b6b1-e06e-421c-9c5c-bfc3b3e3be77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Running the dataset through the scale_images preprocessing step\n",
    "ds = ds.map(scale_images) \n",
    "# Cache the dataset for that batch \n",
    "ds = ds.cache()\n",
    "# Shuffle it up \n",
    "ds = ds.shuffle(60000)\n",
    "# Batch into 128 images per sample\n",
    "ds = ds.batch(128)\n",
    "# Reduces the likelihood of bottlenecking \n",
    "ds = ds.prefetch(64)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a5b08df-7b20-41f4-a8ff-112dface1cb0",
   "metadata": {
    "id": "9a5b08df-7b20-41f4-a8ff-112dface1cb0"
   },
   "source": [
    "# 3. Build Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f66add-a3db-467f-96c3-f87b9f880159",
   "metadata": {
    "id": "38f66add-a3db-467f-96c3-f87b9f880159"
   },
   "source": [
    "### 3.1 Import Modelling Components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "bb72da39-377f-4264-b525-c87f49fb0356",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bring in the sequential api for the generator and discriminator\n",
    "from tensorflow.keras.models import Sequential\n",
    "# Bring in the layers for the neural network\n",
    "from tensorflow.keras.layers import Conv2D, Dense, Flatten, Reshape, LeakyReLU, Dropout, UpSampling2D, ReLU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c40405df-1439-4661-8785-d76698df8152",
   "metadata": {
    "id": "c40405df-1439-4661-8785-d76698df8152"
   },
   "source": [
    "### 3.2 Build Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "5d29d43a-e02a-4031-a0ec-de8aa810c118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_generator(): \n",
    "    model = Sequential()\n",
    "    \n",
    "    # Takes in random values and reshapes it to 25x25x128\n",
    "    model.add(Dense(START_RES*START_RES*128, input_dim=128))\n",
    "    model.add(ReLU())\n",
    "    model.add(Reshape((START_RES,START_RES,128)))\n",
    "    \n",
    "    # Upsampling block 1 \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    # Upsampling block 2 \n",
    "    model.add(UpSampling2D())\n",
    "    model.add(Conv2D(128, 5, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    # Convolutional block 1\n",
    "    model.add(Conv2D(128, 4, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    # Convolutional block 2\n",
    "    model.add(Conv2D(128, 4, padding='same'))\n",
    "    model.add(ReLU())\n",
    "    \n",
    "    # Convolutional layer to get to Three channels\n",
    "    model.add(Conv2D(3, 4, padding='same', activation='sigmoid'))\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "741b0d58-1b9f-4260-8405-dc400c73f843",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a Generator\n",
    "generator = build_generator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259ab9c1-6d6c-49a0-b0c4-f45b7c68f588",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "generator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4e0cb6-d741-4d43-b845-2a8f2615765b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate new Images\n",
    "img = generator.predict(np.random.randn(16,128))\n",
    "# Visualise the Generated Images\n",
    "fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10,10))\n",
    "for r in range(4): \n",
    "    for c in range(4): \n",
    "        ax[r][c].imshow(img[(r+1)*(c+1)-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2415abbf-24ed-4bac-8fb8-12c65017ec22",
   "metadata": {
    "id": "2415abbf-24ed-4bac-8fb8-12c65017ec22"
   },
   "source": [
    "### 3.3 Build Discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "b4e70bcb-cfd5-42bb-aed0-79f19bb38d17",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminator(): \n",
    "    model = Sequential()\n",
    "    \n",
    "    # First Conv Block\n",
    "    model.add(Conv2D(32, 5, input_shape = (IMG_RES,IMG_RES,3)))\n",
    "    model.add(ReLU())\n",
    "\n",
    "    \n",
    "    # Second Conv Block\n",
    "    model.add(Conv2D(64, 5))\n",
    "    model.add(ReLU())\n",
    "  \n",
    "    \n",
    "    # Third Conv Block\n",
    "    model.add(Conv2D(128, 5))\n",
    "    model.add(ReLU())\n",
    "\n",
    "    # Fourth Conv Block\n",
    "    model.add(Conv2D(256, 5))\n",
    "    model.add(ReLU())\n",
    "\n",
    "    \n",
    "    # Flatten then pass to dense layer\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    return model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "7173eb57-250b-4d21-9b37-de842c4552ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "discriminator = build_discriminator()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed6fecbc-f214-4f50-865c-91887b2430e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "discriminator.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cd15246-b40c-4c7a-912d-b88a1c5c463b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing wether Discriminator Can classify Images as Real or fake\n",
    "discriminator.predict(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b343b0-38d3-4281-bedb-72099a18097e",
   "metadata": {
    "id": "39b343b0-38d3-4281-bedb-72099a18097e"
   },
   "source": [
    "# 4. Construct Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884abab3-2f74-442d-856f-e104ef1ac8ef",
   "metadata": {
    "id": "884abab3-2f74-442d-856f-e104ef1ac8ef"
   },
   "source": [
    "### 4.1 Setup Losses and Optimizers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "4e5543e2-c931-47ff-adc0-412e66acfac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adam is going to be the optimizer for both\n",
    "from tensorflow.keras.optimizers import Adam, SGD\n",
    "# Binary cross entropy is going to be the loss for both \n",
    "from tensorflow.keras.losses import BinaryCrossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "198b2d4e-d6b9-4b6c-a98c-65cd1b81da26",
   "metadata": {},
   "outputs": [],
   "source": [
    "g_opt = Adam(learning_rate=0.0001) \n",
    "d_opt = Adam(learning_rate=0.00001) \n",
    "g_loss = BinaryCrossentropy()\n",
    "d_loss = BinaryCrossentropy()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f170b0e-f731-4cbd-8068-24896f462c08",
   "metadata": {
    "id": "9f170b0e-f731-4cbd-8068-24896f462c08"
   },
   "source": [
    "### 4.2 Build Subclassed Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "9e2f5654-ed22-462d-be32-6c43d8b99b74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing the base model class to subclass our training step \n",
    "from tensorflow.keras.models import Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "40a0af46-0243-4396-94d6-c1316d984de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN(Model): \n",
    "    def __init__(self, generator, discriminator, *args, **kwargs):\n",
    "        # Pass through args and kwargs to base class \n",
    "        super().__init__(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for generator and discrimnator\n",
    "        self.generator = generator \n",
    "        self.discriminator = discriminator \n",
    "        \n",
    "    def compile(self, g_opt, d_opt, g_loss, d_loss, *args, **kwargs): \n",
    "        # Compile with base class\n",
    "        super().compile(*args, **kwargs)\n",
    "        \n",
    "        # Create attributes for losses and optimizers\n",
    "        self.g_opt = g_opt\n",
    "        self.d_opt = d_opt\n",
    "        self.g_loss = g_loss\n",
    "        self.d_loss = d_loss \n",
    "\n",
    "    def train_step(self, batch):\n",
    "        # Get the data \n",
    "        real_images = batch\n",
    "        fake_images = self.generator(tf.random.normal((128, 128)), training=False)\n",
    "        \n",
    "        # Train the discriminator\n",
    "        with tf.GradientTape() as d_tape: \n",
    "            # Pass the real and fake images to the discriminator model\n",
    "            yhat_real = self.discriminator(real_images, training=True) \n",
    "            yhat_fake = self.discriminator(fake_images, training=True)\n",
    "            yhat_realfake = tf.concat([yhat_real, yhat_fake], axis=0)\n",
    "            \n",
    "            # Create labels for real and fakes images\n",
    "            y_realfake = tf.concat([tf.zeros_like(yhat_real), tf.ones_like(yhat_fake)], axis=0)\n",
    "            \n",
    "            # Add some noise to the TRUE outputs\n",
    "            noise_real = 0.15*tf.random.uniform(tf.shape(yhat_real))\n",
    "            noise_fake = -0.15*tf.random.uniform(tf.shape(yhat_fake))\n",
    "            y_realfake += tf.concat([noise_real, noise_fake], axis=0)\n",
    "            \n",
    "            # Calculate loss - BINARYCROSS \n",
    "            total_d_loss = self.d_loss(y_realfake, yhat_realfake)\n",
    "            \n",
    "        # Apply backpropagation\n",
    "        dgrad = d_tape.gradient(total_d_loss, self.discriminator.trainable_variables) \n",
    "        self.d_opt.apply_gradients(zip(dgrad, self.discriminator.trainable_variables))\n",
    "        \n",
    "        # Train the generator \n",
    "        with tf.GradientTape() as g_tape: \n",
    "            # Generate some new images\n",
    "            gen_images = self.generator(tf.random.normal((128,128)), training=True)\n",
    "                                        \n",
    "            # Create the predicted labels\n",
    "            predicted_labels = self.discriminator(gen_images, training=False)\n",
    "                                        \n",
    "            # Calculate loss - trick to training to fake out the discriminator\n",
    "            total_g_loss = self.g_loss(tf.zeros_like(predicted_labels), predicted_labels) \n",
    "            \n",
    "        # Apply backpropagation\n",
    "        ggrad = g_tape.gradient(total_g_loss, self.generator.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(ggrad, self.generator.trainable_variables))\n",
    "        \n",
    "        return {\"d_loss\":total_d_loss, \"g_loss\":total_g_loss}\n",
    "\n",
    "    def set_discrimantor(self, discriminator):\n",
    "        self.discriminator = discriminator\n",
    "\n",
    "    def set_generator(self, generator):\n",
    "        self.generator = generator\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "24d248c3-f4c1-4478-a699-a5811a7b1fd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create instance of subclassed model\n",
    "gan = GAN(generator, discriminator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "e1cf7e02-ee1a-4901-bdf0-9aa2301f8cfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile the model\n",
    "gan.compile(g_opt, d_opt, g_loss, d_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06d0adb-38d0-4558-b824-7416cf880082",
   "metadata": {
    "id": "e06d0adb-38d0-4558-b824-7416cf880082"
   },
   "source": [
    "### 4.3 Build Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "548f6918-366c-4799-9dac-1acedaab40c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.image import array_to_img\n",
    "from tensorflow.keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "d3e2bb77-2d7d-40d0-809f-526b8fd34170",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure the directory exists to save plots\n",
    "images_dir = 'Images'\n",
    "os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "class ModelMonitor(Callback):\n",
    "    def __init__(self, num_img=1, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.uniform((self.num_img, self.latent_dim,1))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images *= 255\n",
    "        generated_images.numpy()\n",
    "        for i in range(self.num_img):\n",
    "            img = array_to_img(generated_images[i])\n",
    "            img.save(os.path.join(images_dir, f'generated_img_{epoch}_{i}.png'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16e2f159-25e7-4e35-95ef-f0fd18ac5897",
   "metadata": {
    "id": "16e2f159-25e7-4e35-95ef-f0fd18ac5897"
   },
   "source": [
    "### 4.3 Train "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a779dceb-aba6-4bf3-af49-0d32a76dd2f7",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Recommend 2000 epochs\n",
    "hist = gan.fit(ds, epochs=500, callbacks=[ModelMonitor()])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39c665a1-a4cc-41ac-a08a-2e14ba64e88d",
   "metadata": {
    "id": "39c665a1-a4cc-41ac-a08a-2e14ba64e88d"
   },
   "source": [
    "### 4.4 Review Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54381e8c-93ee-4022-9df6-24c4356720fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.suptitle('Loss')\n",
    "plt.plot(hist.history['d_loss'], label='d_loss')\n",
    "plt.plot(hist.history['g_loss'], label='g_loss')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d319a982-7ae5-4754-adcf-b490f17a79d6",
   "metadata": {
    "id": "d319a982-7ae5-4754-adcf-b490f17a79d6"
   },
   "source": [
    "# 5. Test Out the Generator"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29",
   "metadata": {
    "id": "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29"
   },
   "source": [
    "### 5.1 Generate Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c46f3d6a-8aa5-40d2-a5ac-67a0606a82f0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#generator.load_weights('generator.keras')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cde11f-cb26-4ebf-ad04-2c64a54f871e",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = generator.predict(tf.random.normal((16, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f745982f-c4d7-451f-91a7-f7c4341cb7b7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10,10))\n",
    "for r in range(4): \n",
    "    for c in range(4): \n",
    "        ax[r][c].imshow(imgs[(r+1)*(c+1)-1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5137cffa-784d-4076-beef-0a067b86d3aa",
   "metadata": {
    "id": "5137cffa-784d-4076-beef-0a067b86d3aa"
   },
   "source": [
    "### 5.2 Save the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7011d68-ef71-4377-91e2-e26a02fab382",
   "metadata": {},
   "outputs": [],
   "source": [
    "#generator.save('generator.keras')\n",
    "#discriminator.save('discriminator.keras')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e85fa74e-8e28-41bf-b8ec-d473329d3444",
   "metadata": {},
   "source": [
    "# 6. Scale Images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "014bc70b-d088-45b9-90dc-b7a95e836cc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm  # Import tqdm for progress bar\n",
    "# Source and target directories\n",
    "source_dir = 'Images\\\\R-1'\n",
    "target_dir = 'Images\\\\R-1-scalled-1080'\n",
    "\n",
    "# Ensure target directory exists\n",
    "os.makedirs(target_dir, exist_ok=True)\n",
    "\n",
    "# List all files in the source directory\n",
    "file_list = os.listdir(source_dir)\n",
    "\n",
    "# Initialize tqdm with total number of files\n",
    "progress_bar = tqdm(total=len(file_list), desc='Processing Images', unit='image')\n",
    "\n",
    "# Process each image file\n",
    "for filename in file_list:\n",
    "    if filename.endswith('.png') or filename.endswith('.jpg'):\n",
    "        # Read image file\n",
    "        image_path = os.path.join(source_dir, filename)\n",
    "        image = tf.io.read_file(image_path)\n",
    "        image = tf.image.decode_image(image, channels=3)  # Automatically detects image format (PNG or JPEG)\n",
    "\n",
    "        # Define target size for upscaling\n",
    "        target_height = 1080\n",
    "        target_width = 1080\n",
    "\n",
    "        # Resize image using bilinear interpolation\n",
    "        upscaled_image = tf.image.resize(image, [target_height, target_width], method=tf.image.ResizeMethod.BILINEAR)\n",
    "        upscaled_image = tf.cast(upscaled_image, tf.uint8)\n",
    "\n",
    "        # Save upscaled image to target directory with the same filename\n",
    "        target_image_path = os.path.join(target_dir, filename)\n",
    "        tf.io.write_file(target_image_path, tf.io.encode_png(upscaled_image))  # Adjust encode method as needed\n",
    "\n",
    "        # Update tqdm progress bar\n",
    "        progress_bar.update(1)\n",
    "\n",
    "# Close tqdm progress bar\n",
    "progress_bar.close()\n",
    "\n",
    "print('Upscaling complete.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d2a393c-96d8-4289-81c0-82904c2db9cd",
   "metadata": {},
   "source": [
    "# 7. Reload and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "252ee515-55ee-43f9-b982-42e1d0da9a2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = tf.keras.models.load_model(\"generator.keras\")\n",
    "discriminator = tf.keras.models.load_model(\"discriminator.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2aa57a50-6845-4da3-82a0-ff701445e44b",
   "metadata": {},
   "outputs": [],
   "source": [
    "imgs = generator.predict(tf.random.normal((16, 128)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66fbd917-9713-495e-93d9-d5a73f14bc79",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(ncols=4, nrows=4, figsize=(10,10))\n",
    "for r in range(4): \n",
    "    for c in range(4): \n",
    "        ax[r][c].imshow(imgs[(r+1)*(c+1)-1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "background_execution": "on",
   "collapsed_sections": [
    "206ba81f-978a-4c31-9c3d-6ebe5a5bfc29"
   ],
   "name": "FashionGAN.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
